% \documentclass[sigconf,natbib=false,authordraft]{acmart}
\documentclass[sigconf,natbib=false]{acmart}

% References
\usepackage[style=ACM-Reference-Format,backend=bibtex,sorting=none]{biblatex}
\addbibresource{references.bib}

\usepackage{booktabs}

% ACM License Text
\setcopyright{acmcopyright}

% DOI
% \acmDOI{10.475/123_4}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

% Conference Details
% \acmConference[WOODSTOCK'19]{ACM Woodstock conference}{July 2019}{El Paso, Texas USA}

% Year
\acmYear{2020}

% Copyright Year
\copyrightyear{2020}

\title{Comparison of CORD-19-trained GPT-2-based Chatbot Responses Across
  Different Text Semantic Similarity Approaches}

\author{David Oniani}
\affiliation{%
  \institution{Mayo Clinic}
  \city{Rochester}
  \state{MN}
  \country{USA}}
\email{oniani.david@mayo.edu}

\author{Dr.~Yanshan Wang}
\affiliation{%
  \institution{Mayo Clinic}
  \city{Rochester}
  \state{MN}
  \country{USA}}
\email{wang.yanshan@mayo.edu}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

  On March 16 (2020), per request of the White House Office of Science and
  Technology Policy, new COVID-19 machine readable dataset
  (CORD-19)~\cite{whitehousecovid2020} has been released. We have utilized 774M
  GPT-2~\cite{radford2019language} model and applied transfer learning to
  retrain the model on this corpus. Ultimately, we have created a COVID-19
  conversational chatbot. In order to improve the performance of the chatbot,
  we have applied 4 different text semantic similarity techniques using
  pretrained models including BERT~\cite{turc2019}, BioBERT~\cite{btz682}, and
  Universal Sentence Encoder (USE)~\cite{use} as additional layers on top of
  GPT-2-based chatbot responses. We present the results annotated by
  experienced medical personnel at Mayo Clinic.

\end{abstract}

\keywords{gpt-2, covid-19, cord-19, bert, biobert, universal sentence encoder,
  dataset, nlp, ai, semantic similarity}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Chatbots have been much studied in recent years and with the advancements in
the fields of artificial intelligence and natural language processing, both the
the functionality and the performance have seen drastic improvements. Semantic
similarity of texts, on the other hand, has been studied for a long time and
recent breakthroughs allowed for development of new models such as BERT,
BioBERT, and Universal Sentence Encoder. The paper takes an approach which is a
marriage of these two and brings a different perspective on the chatbot
creation. We first let a human ask a question and make GPT-2 come up with an
answer. Then we further process the answer with additional filters and
ultimately, apply a different model for finding the sentences that are most
relevant to the question.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dataset and Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Corpus}

We harvested the data from the initial \textit{commercial use subset} of
COVID-19 Open Research Dataset (CORD-19)~\cite{cord19} containing 9000
scholarly articles in the form of JSON files. We extracted the abstract and the
main body of the article from every JSON file, combined them together, and used
as a corpus for retraining the GPT-2 model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Transfer Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transfer Learning}

We utilized GPT-2 774M model and ran transfer learning for 2500 iterations with
the batch size of 8. We used Adam as the optimizer and set the learning rate of
0.0001. The model is available on our GitHub page.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semantic Similarity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Semantic Similarity}

The GPT-2 responses are usually very lengthy and for the most part, the answer
is not relevant to the question. To solve this problem, we chunked the answer
into separate sentences and found the ones that are most \textit{semantically
  similar} to the question asked. For this task, we have tested and applied 4
different approaches:

\begin{itemize}
  \item BioBERT large v1.1 (+PubMed 1M) model based on BERT-large Cased (custom 30k vocabulary)
  \item Universal Sentence Encoder (USE), version 3, large
  \item Bert-Large, uncased (24 layers and 340M parameters)
  \item Scikit learn's Tfidfvectorizer
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Questions and Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Questions and Evaluation}

For evaluating the performance of the approaches, we chose 12 different
questions (presented below).

\begin{itemize}
  \item Are there geographic variations in the mortality rate of COVID-19?

  \item What is known about transmission, incubation, and environmental
        stability of COVID-19?

  \item Is there any evidence to suggest geographic based virus mutations of
        COVID-19?

  \item Are there geographic variations in the rate of COVID-19 spread?

  \item What do we know about virus genetics, origin, and evolution of
        COVID-19?

  \item What has been published about ethical and social science considerations
        of COVID-19?

  \item What has been published about medical care of COVID-19?

  \item What do we know about diagnostics and surveillance of COVID-19?

  \item What do we know about COVID-19 risk factors?

  \item What has been published about information sharing and inter-sectoral
        collaboration of COVID-19?

  \item What do we know about vaccines and therapeutics of COVID-19?

  \item What do we know about non-pharmaceutical interventions of COVID-19?
\end{itemize}

\noindent For each approach, we generated 5 different answers for the same
question, resulting in the total of 240 answers. We then asked experienced
medical personnel at Mayo Clinic to rate these answers. The rating system was
composed of 5 different scores:

\begin{itemize}
  \item	Relevant --- the answer partially or fully answers the question and/or
        makes clear attempts to do so and is related to the question.
        (5 points)

  \item	Well-formed --- the answer makes a logical sense and is somewhat
        related to both the question and COVID-19, yet it does not (partially
        or fully) answer the question.
        (4 points)

  \item	Informative --- the answer is not related to the question, but provides
        some information about COVID-19 and makes a logical sense.
        (3 points)

  \item	Acceptable --- the answer makes some logical sense and is weakly
        related to the question or COVID-19, but is mostly difficult to
        understand.
        (2 points)

  \item	Poor --- the answer is totally unrelated to the question or COVID-19
        and/or does not make a logical sense.
        (1 point)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

\begin{figure}[H]
  \centering
  \begin{tabular}{*{2}{c}}
    \toprule
    Approach & Score\\
    \midrule
    TfidfVectorizer + Cosine Similarity & TBD\\
    \midrule
    BERT + Cosine Similarity & TBD\\
    \midrule
    BioBERT + Cosine Similarity & TBD\\
    \midrule
    Universal Sentence Encoder (USE) + Inner Product & TBD\\
    \bottomrule
  \end{tabular}
  \caption{Comparison of 4 Different Approaches.}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean porta purus et
sem gravida rutrum. Maecenas blandit nulla ac luctus tempus. Nam finibus
posuere ante, et lacinia massa vestibulum sit amet. Nulla velit arcu, efficitur
quis turpis nec, sollicitudin lobortis nisi. Vivamus ut diam ut eros faucibus
fringilla. Suspendisse pellentesque magna nec velit tristique sollicitudin.
Morbi ultrices nec augue et molestie. Nam sapien ante, ullamcorper elementum
convallis id, faucibus in lectus. Fusce pellentesque mollis velit efficitur
porta. Sed finibus ligula quam, et lacinia velit posuere auctor. Donec ligula
lorem, dictum nec lectus in, vehicula tincidunt massa. In hac habitasse platea
dictumst.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography%

\end{document}
