% \documentclass[sigconf,natbib=false,authordraft]{acmart}
\documentclass[sigconf,natbib=false]{acmart}

% References
\usepackage[style=ACM-Reference-Format,backend=bibtex,sorting=none]{biblatex}
\addbibresource{references.bib}

\usepackage{booktabs}

% ACM License Text
\setcopyright{acmcopyright}

% DOI
% \acmDOI{10.475/123_4}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

% Conference Details
% \acmConference[WOODSTOCK'19]{ACM Woodstock conference}{July 2019}{El Paso, Texas USA}

% Year
\acmYear{2020}

% Copyright Year
\copyrightyear{2020}

\title{Comparison of CORD-19-trained GPT-2-based Chatbot Responses Across
  Different Text Semantic Similarity Approaches}

\author{David Oniani}
\affiliation{%
  \institution{Mayo Clinic}
  \city{Rochester}
  \state{MN}
  \country{USA}}
\email{oniani.david@mayo.edu}

\author{Dr.~Yanshan Wang}
\affiliation{%
  \institution{Mayo Clinic}
  \city{Rochester}
  \state{MN}
  \country{USA}}
\email{wang.yanshan@mayo.edu}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

  On March 16 (2020), per request of the White House Office of Science and
  Technology Policy, new COVID-19 machine readable dataset
  (CORD-19)~\cite{whitehousecovid2020} has been released. We have utilized 774M
  GPT-2~\cite{radford2019language} model and applied transfer learning to
  retrain the model on this corpus. Ultimately, we have created a COVID-19
  conversational chatbot. In order to improve the performance of the chatbot,
  we have applied 4 different text semantic similarity techniques using
  pretrained models including BERT~\cite{turc2019}, BioBERT~\cite{btz682}, and
  Universal Sentence Encoder (USE)~\cite{use} as additional layers on top of
  GPT-2-based chatbot responses. We present the results annotated by
  experienced medical personnel at Mayo Clinic.

\end{abstract}

\keywords{gpt-2, covid-19, cord-19, bert, biobert, universal sentence encoder,
  dataset, nlp, ai, semantic similarity}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Chatbots have been much studied in recent years and with the advancements in
the fields of artificial intelligence and natural language processing, both the
the functionality and the performance have seen drastic improvements. Semantic
similarity of texts, on the other hand, has been studied for a long time and
recent breakthroughs allowed for development of new models such as BERT,
BioBERT, and Universal Sentence Encoder. The paper takes an approach which is a
marriage of these two and brings a different perspective on the chatbot
creation. We first let a human ask a question and make GPT-2 come up with an
answer. Then we further process the answer with additional filters and
ultimately, apply a different model for finding the sentences that are most
relevant to the question.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dataset and Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Corpus}

We harvested the data from the initial \textit{commercial use subset} of
COVID-19 Open Research Dataset (CORD-19)~\cite{cord19} containing 9000
scholarly articles in the form of JSON files. We extracted the abstract and the
main body of the article from every JSON file, combined them together, and used
as a corpus for retraining the GPT-2 model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Transfer Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transfer Learning}

We used GPT-2 774M model and ran transfer learning for 2500 iterations with the
batch size of 8. We used Adam as the optimizer and set the learning rate of
0.0001. The model is available on our GitHub page.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semantic Similarity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Semantic Similarity}

The GPT-2 responses are usually very lengthy and for the most part, the answer
is not relevant to the question. To solve this problem, we chunked the answer
into separate sentences and found the ones that are most \textit{semantically
  similar} to the question asked. For this task, we have tested and applied 4
different approaches:

\begin{itemize}
  \item BioBERT large v1.1 (+PubMed 1M) model based on BERT-large Cased (custom 30k vocabulary)
  \item Universal Sentence Encoder (USE), version 3, large
  \item Bert-Large, uncased (24 layers and 340M parameters)
  \item Scikit learn's Tfidfvectorizer
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

We have generated over 300 answers to the questions. We have asked experienced
medical personnel at Mayo Clinic to rate our question.

\begin{figure}[H]
  \centering
  \begin{tabular}{*{2}{c}}
    \toprule
    Approach & Score\\
    \midrule
    TfidfVectorizer + Cosine Similarity & TBD\\
    \midrule
    BERT + Cosine Similarity & TBD\\
    \midrule
    BioBERT + Cosine Similarity & TBD\\
    \midrule
    Universal Sentence Encoder (USE) + Inner Product & TBD\\
    \bottomrule
  \end{tabular}
  \caption{Comparison of 4 Different Approaches.}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean porta purus et
sem gravida rutrum. Maecenas blandit nulla ac luctus tempus. Nam finibus
posuere ante, et lacinia massa vestibulum sit amet. Nulla velit arcu, efficitur
quis turpis nec, sollicitudin lobortis nisi. Vivamus ut diam ut eros faucibus
fringilla. Suspendisse pellentesque magna nec velit tristique sollicitudin.
Morbi ultrices nec augue et molestie. Nam sapien ante, ullamcorper elementum
convallis id, faucibus in lectus. Fusce pellentesque mollis velit efficitur
porta. Sed finibus ligula quam, et lacinia velit posuere auctor. Donec ligula
lorem, dictum nec lectus in, vehicula tincidunt massa. In hac habitasse platea
dictumst.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography%

\end{document}
